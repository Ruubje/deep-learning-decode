{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:10:58.369679Z",
     "start_time": "2020-10-10T13:10:54.449771Z"
    }
   },
   "outputs": [],
   "source": [
    "import decode\n",
    "import decode.utils\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODE - Training\n",
    "\n",
    "This notebook highlights how to train a DECODE model for fitting experimental data.\n",
    "\n",
    "The DECODE network is trained on simulated data. This requires a calibration spline based PSF model,\n",
    "and a parameter file that contains camera settings and other hyper parameters to set up the simulation process.\n",
    "\n",
    "While we provide an example calibration file here to demonstrate the training procedure you will have to create one yourself for your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "Set device for training. We **do not recommend training on CPU** since this will be quite slow. If you train on CPU though, you may want to change the number of threads if you have a big machine (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.716133Z",
     "start_time": "2020-10-10T16:31:53.711877Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda'  # or 'cpu'\n",
    "device_ix = 0  # possibly change device index (only for cuda)\n",
    "threads = 4  #  number of threads, useful for CPU heavy computation. Change if you know what you are doing.\n",
    "worker = 4  # number of workers for data loading. Change only if you know what you are doing.\n",
    "\n",
    "torch.set_num_threads(threads)  # set num threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load example calibration file from server (this can be skipped)\n",
    "gateway = decode.utils.example_helper.load_gateway()\n",
    "\n",
    "# dir where to store example data, leave as '' to store in current folder\n",
    "path = Path('')\n",
    "\n",
    "# change here for other files\n",
    "package = gateway['examples']['package_1']\n",
    "\n",
    "# get paths to files\n",
    "zip_folder = decode.utils.example_helper.load_example_package(\n",
    "    path=(path / package['name']).with_suffix('.zip'), url=package['url'], hash=package['hash'])\n",
    "\n",
    "calib_file = str(zip_folder / 'spline_calibration_3dcal.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bead calibration\n",
    "\n",
    "To obtain the calibration file a spline model is fit on a stack of beads.     \n",
    "We recommend to use fit3Dspline which is part of the SMAP package.\n",
    "\n",
    "1. Install the stand-alone version of SMAP from [www.rieslab.de](www.rieslab.de) or if you have Matlab, downlowd the source-code from [www.github.com/jries/SMAP](www.github.com/jries/SMAP). There, you also find the installation instructions and Documentation.\n",
    "2. Acquire z-stacks with fluorescent beads (e.g. 100 nm beads). We typcally use a z-range of +/- 750 nm and a step size of 10-50 nm.\n",
    "3. In SMAP, use the plugin *calibrate3DSplinePSF* to generate the calibartion file. In the user guide (accessible from the SMAP help menu) in section 5.4, this is explained in detail. Further information about the calibration process can be found in [Li et al, Nature Methods (2018)](https://doi.org/10.1038/nmeth.4661)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:10:58.374538Z",
     "start_time": "2020-10-10T13:10:58.371804Z"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment and exectue only if you have not executed the example download cell above\n",
    "# calib_file = 'examples/2020-07-23_Pos0_beads_Z_1_MMStack_Default.ome_3dcal.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the simulation parameters we load the default config file and go through the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:10:58.422278Z",
     "start_time": "2020-10-10T13:10:58.376603Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy default parameter files which are then changed as specified\n",
    "decode.utils.param_io.copy_reference_param('')  # saved in current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:10:58.422278Z",
     "start_time": "2020-10-10T13:10:58.376603Z"
    }
   },
   "outputs": [],
   "source": [
    "param = decode.utils.param_io.load_params('param_friendly.yaml')  # change path if you load custom file\n",
    "\n",
    "param.Hardware.device = device\n",
    "param.Hardware.device_ix = device_ix\n",
    "param.Hardware.device_simulation = device\n",
    "param.Hardware.torch_threads = threads\n",
    "param.Hardware.num_worker_train = worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The camera parameters need to be adjusted according to the device used. Here we used an EMCCD camera, for a sCMOS device you must set the em_gain to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:10:58.442152Z",
     "start_time": "2020-10-10T13:10:58.430385Z"
    }
   },
   "outputs": [],
   "source": [
    "param.Camera.baseline = 398.6\n",
    "param.Camera.e_per_adu = 5.0\n",
    "param.Camera.em_gain = 100\n",
    "param.Camera.px_size =[127.0, 117.0] # Pixel Size in nano meter\n",
    "param.Camera.qe = 1.0                # Quantum efficiency\n",
    "param.Camera.read_sigma = 58.8\n",
    "param.Camera.spur_noise = 0.0015\n",
    "\n",
    "param.Camera.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation parameters should be set so that the resulting simulated frames resemble real frames as closely as possible.\n",
    "You can use SMAP to infer these parameters by performing inference with an iterative approach on a couple of frames:\n",
    "\n",
    "1. Use the bead calibration to fit your SMLM data.\n",
    "2. Use the plugin: *DECODE\\_training\\_estimates* to estimate the photo-physical parameters of the experiment and to save them into a parameter file. Consult the information of the plugin (accessible via the Info button) for further information.\n",
    "\n",
    "However it is also possible to find reasonable values by hand which we do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:10:58.989582Z",
     "start_time": "2020-10-10T13:10:58.979739Z"
    }
   },
   "outputs": [],
   "source": [
    "param.Simulation.bg_uniform = [20.0, 200.0]           # background range to sample from. You can also specify a const. value as 'bg_uniform = 100'\n",
    "param.Simulation.emitter_av = 25                      # Average number of emitters per frame\n",
    "param.Simulation.emitter_extent[2] = [-800, 800]    # Volume in which emitters are sampled. x,y values should not be changed. z-range (in nm) should be adjusted according to the PSF\n",
    "param.Simulation.intensity_mu_sig = [7000.0, 3000.0]  # Average intensity and its standard deviation\n",
    "param.Simulation.lifetime_avg = 1.                     # Average lifetime of each emitter in frames. A value between 1 and 2 works for most experiments\n",
    "\n",
    "param.Simulation.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we provide the path to the calibration file, and to the output destination.\n",
    "There are more parameters (you can just execute param in a cell to look at them) that you should not need to change.   \n",
    "You might have to reduce the batch size (param.HyperParameter.batch_size) if you run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:11:00.136401Z",
     "start_time": "2020-10-10T13:11:00.130209Z"
    }
   },
   "outputs": [],
   "source": [
    "param.InOut.calibration_file = calib_file\n",
    "param.InOut.experiment_out = ''\n",
    "\n",
    "param.InOut.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up our simulator and the camera model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:37:13.891159Z",
     "start_time": "2020-10-07T17:37:13.353460Z"
    }
   },
   "outputs": [],
   "source": [
    "simulator, sim_test = decode.neuralfitter.train.live_engine.setup_random_simulation(param)\n",
    "camera = decode.simulation.camera.Photon2Camera.parse(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:15:57.464446Z",
     "start_time": "2020-10-10T13:15:57.460660Z"
    }
   },
   "outputs": [],
   "source": [
    "# finally we derive some parameters automatically for easy use\n",
    "param = decode.utils.param_io.autoset_scaling(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us sample a set frames, and also load our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:37:19.712616Z",
     "start_time": "2020-10-07T17:37:15.484074Z"
    }
   },
   "outputs": [],
   "source": [
    "tar_em, sim_frames, bg_frames = simulator.sample()\n",
    "sim_frames = sim_frames.cpu()\n",
    "\n",
    "frame_path = zip_folder / 'frames.tif'  # change if you load your own data\n",
    "data_frames = decode.utils.frames_io.load_tif(frame_path).cpu()\n",
    "\n",
    "print(f'Data shapes, simulation: {sim_frames.shape}, real data: {data_frames.shape}')\n",
    "print(f'Average value, simulation: {sim_frames.mean().round()}, real data: {data_frames.mean().round()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the mean brightness, we see that there is a large missmatch. The reason is that we simulate the photons that are emitted by the fluorophores, while the tiff file shows the photons that are recorded by the camera (after amplificiation). To enable direct comparison we convert the real data frames into photon numbers.   \n",
    "Be aware that camera.forward()  and camera.forward() are not each others inverse. forward() performs noise sampling, while backward() simply rescales by the em_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:23:28.126478Z",
     "start_time": "2020-10-07T17:23:26.387702Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frames = camera.backward(data_frames, device='cpu')\n",
    "print(f'Average value, simulation: {sim_frames.mean().round()}, real data: {data_frames.mean().round()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T20:50:07.906886Z",
     "start_time": "2020-10-05T20:50:07.902719Z"
    }
   },
   "source": [
    "By comparing random frames (chosing a dense region of the real data) we can convince ourselves that the distributions are somewhat similar.   \n",
    "If you observe large differences for your dataset, you probably have to adjust param.Simulation.bg_uniform and param.Simulation.intensity_mu_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-07T17:23:37.724720Z",
     "start_time": "2020-10-07T17:23:37.107083Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(sim_frames[np.random.randint(0,len(sim_frames))])\n",
    "plt.colorbar()\n",
    "plt.subplot(122)\n",
    "plt.imshow(data_frames[np.random.randint(0,len(data_frames)), 30:70,-40:])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are happy with you settings you can write the parameters to a file (or edit the param_friendly.yaml directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T13:16:00.655365Z",
     "start_time": "2020-10-10T13:16:00.630594Z"
    }
   },
   "outputs": [],
   "source": [
    "param_out_path = 'notebook_example.yaml' # or an alternative path\n",
    "decode.utils.param_io.save_params(param_out_path, param)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T21:00:50.260177Z",
     "start_time": "2020-10-05T21:00:50.254454Z"
    }
   },
   "source": [
    "# Start the Actual Training of the Model\n",
    "To start training we recommend to start it from a new terminal window/Anaconda prompt as this is most stable. So please open up a new terminal, activate the respective environment and start the training.\n",
    "\n",
    "    conda activate decode_env\n",
    "    cd [directory where this notebook is]\n",
    "    python -m decode.neuralfitter.train.live_engine -p notebook_example.yaml  # change path if you modified it\n",
    "    \n",
    "In case you experience multiprocessing issues, please consult our FAQ (https://decode.readthedocs.io/en/master/faq.html). You may run the command above with the additional `-w 0` flag.\n",
    "\n",
    "To check training progress you may start a tensorboard instance. Again, please open a new terminal instancePlease navigate (within the terminal, in a new terminal window/tab) to the directory of this notebook and start tensorboard. Don't forget to activate the conda environment before starting tensorboard.\n",
    "\n",
    "    cd [directory which containts 'runs', i.e. this notebook's path]\n",
    "    conda activate decode_env\n",
    "    tensorboard --samples_per_plugin images=100 --port=6006 --logdir=runs\n",
    "\n",
    "Open it in your browser: http://localhost:6006\n",
    "Numereous evaluation metrics are tracked here. Under the 'Images' header you can see example frames and localizations.   \n",
    "Keep in mind though that these are all calculated on the simulated data. So you can check whether the training has converged, \n",
    "but you'll have to run the model on the actual data to see if you set the parameters correctly.\n",
    "\n",
    "For that open the Fit.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_env_rc4]",
   "language": "python",
   "name": "conda-env-decode_env_rc4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
