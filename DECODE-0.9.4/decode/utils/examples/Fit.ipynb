{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODE - Fit SMLM Data\n",
    "The purpose of this notebook is to demonstrate the fitting procedure for a pretrained model.\n",
    "Please be advised to have a read of the Introduction notebook first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.707922Z",
     "start_time": "2020-10-10T16:31:49.338797Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import decode\n",
    "import decode.utils\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "Set device for inference (i.e. CUDA vs. CPU, for our setup inference on the CPU is about 10 times slower). If you fit on CPU though, you may want to change the number of threads if you have a big machine (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.716133Z",
     "start_time": "2020-10-10T16:31:53.711877Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'  # or 'cpu', or you change cuda device index\n",
    "threads = 4  #  number of threads, useful for CPU heavy computation. Change if you know what you are doing.\n",
    "worker = 4  # number of workers for data loading. Change only if you know what you are doing.\n",
    "\n",
    "torch.set_num_threads(threads)  # set num threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Specify paths for the model, parameters and frames\n",
    "If you trained you own model (using the Training notebook) you can specify it's path here. \n",
    "\n",
    "**Important** If the camera parameters of the training differ from the data which should be fitted (e.g. different EM gain), you can try to use the model anyways, but you must specify them here since we convert to photon units before forwarding through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.730382Z",
     "start_time": "2020-10-10T16:31:53.719167Z"
    }
   },
   "outputs": [],
   "source": [
    "param_path = 'config/notebook_example.yaml'\n",
    "model_path = '2020-10-07_14-11-51_turagas-ws4/model_0.pt'\n",
    "frame_path = 'package_1/frames.tif'\n",
    "\n",
    "# specify camera parameters of tiffs\n",
    "meta = {\n",
    "    'Camera': {\n",
    "        'baseline': 398.6,\n",
    "        'e_per_adu': 5.0,\n",
    "        'em_gain': 50.0,\n",
    "        'spur_noise': 0.0015  # if you don't know, you can set this to 0.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T14:36:41.999701Z",
     "start_time": "2020-10-10T14:36:41.990217Z"
    }
   },
   "source": [
    "Alternatively if you just want to check out the fitting procedure we provide several example data for trying out DECODE. For this we load a gateway file which includes the links to the respective data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:53.740374Z",
     "start_time": "2020-10-10T16:31:53.733195Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gateway = decode.utils.example_helper.load_gateway()\n",
    "\n",
    "# dir where to store example data, leave as '' to store in current folder\n",
    "path = Path('')\n",
    "\n",
    "# change here for other files\n",
    "package = gateway['examples']['package_1']\n",
    "\n",
    "# get paths to files\n",
    "zip_folder = decode.utils.example_helper.load_example_package(\n",
    "    path=(path / package['name']).with_suffix('.zip'), url=package['url'], hash=package['hash'])\n",
    "\n",
    "frame_path = zip_folder / 'frames.tif'\n",
    "meta_path = zip_folder / 'meta.yaml'\n",
    "model_path = zip_folder / 'model.pt'\n",
    "param_path = zip_folder / 'param_run.yaml'\n",
    "\n",
    "# load meta information (em gain of tif etc.)\n",
    "with meta_path.open('r') as f:\n",
    "    meta = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Parameters and Model\n",
    "Specify Post-Processing as by the parameter file you trained the model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:55.723628Z",
     "start_time": "2020-10-10T16:31:53.743152Z"
    }
   },
   "outputs": [],
   "source": [
    "param = decode.utils.param_io.load_params(param_path)\n",
    "model = decode.neuralfitter.models.SigmaMUNet.parse(param)\n",
    "model = decode.utils.model_io.LoadSaveModel(model,\n",
    "                                            input_file=model_path,\n",
    "                                            output_file=None).load_init(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:31:55.723628Z",
     "start_time": "2020-10-10T16:31:53.743152Z"
    }
   },
   "outputs": [],
   "source": [
    "# overwrite camera\n",
    "param = decode.utils.param_io.autofill_dict(meta['Camera'], param.to_dict(), mode_missing='include')\n",
    "param = decode.utils.param_io.RecursiveNamespace(**param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Frames\n",
    "Load TIFF file.\n",
    "Change only needed if you load a custom tif file.\n",
    "Note that the TIFF loader will auto-load and concatenate tiff files\n",
    "that are consecutive and share\n",
    "the same meta data. For example specifying `dummy.tif` will also load  `dummy_0.tif, dummy_1.tif` if present in the\n",
    "same folder.\n",
    "If you have single page tiff files, you can also specify a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:43:05.840921Z",
     "start_time": "2020-10-10T16:43:03.773855Z"
    }
   },
   "outputs": [],
   "source": [
    "# depends on your input, e.g. load a tiff\n",
    "frames = decode.utils.frames_io.load_tif(frame_path)\n",
    "\n",
    "camera = decode.simulation.camera.Photon2Camera.parse(param)\n",
    "camera.device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Pre- and Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:35:47.813345Z",
     "start_time": "2020-10-10T16:35:45.926295Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup frame processing as by the parameter with which the model was trained\n",
    "frame_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "    decode.neuralfitter.utils.processing.wrap_callable(camera.backward),\n",
    "    decode.neuralfitter.frame_processing.AutoCenterCrop(8),\n",
    "    decode.neuralfitter.frame_processing.Mirror2D(dims=-1),\n",
    "    decode.neuralfitter.scale_transform.AmplitudeRescale.parse(param)\n",
    "])\n",
    "\n",
    "# determine extent of frame and its dimension after frame_processing\n",
    "size_procced = decode.neuralfitter.frame_processing.get_frame_extent(frames.unsqueeze(1).size(), frame_proc.forward)  # frame size after processing\n",
    "frame_extent = ((-0.5, size_procced[-2] - 0.5), (-0.5, size_procced[-1] - 0.5))\n",
    "\n",
    "\n",
    "# Setup post-processing\n",
    "# It's a sequence of backscaling, relative to abs. coord conversion and frame2emitter conversion\n",
    "post_proc = decode.neuralfitter.utils.processing.TransformSequence([\n",
    "    \n",
    "    decode.neuralfitter.scale_transform.InverseParamListRescale.parse(param),\n",
    "    \n",
    "    decode.neuralfitter.coord_transform.Offset2Coordinate(xextent=frame_extent[0],\n",
    "                                                          yextent=frame_extent[1],\n",
    "                                                          img_shape=size_procced[-2:]),\n",
    "    \n",
    "    decode.neuralfitter.post_processing.NMSPostProcessing(raw_th=0.1,\n",
    "                                                          xy_unit='px', \n",
    "                                                          px_size=param.Camera.px_size)\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:39:34.382654Z",
     "start_time": "2020-10-10T16:35:49.690927Z"
    }
   },
   "outputs": [],
   "source": [
    "infer = decode.neuralfitter.Infer(model=model, ch_in=param.HyperParameter.channels_in,\n",
    "                                  frame_proc=frame_proc, post_proc=post_proc,\n",
    "                                  device=device, num_workers=worker)\n",
    "\n",
    "emitter = infer.forward(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:39:34.494270Z",
     "start_time": "2020-10-10T16:39:34.392752Z"
    }
   },
   "outputs": [],
   "source": [
    "# check on the output\n",
    "print(emitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if the predictions look reasonable on a random frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:39:35.133481Z",
     "start_time": "2020-10-10T16:39:34.496432Z"
    }
   },
   "outputs": [],
   "source": [
    "random_ix = torch.randint(frames.size(0), size=(1, )).item()\n",
    "\n",
    "em_subset = emitter.get_subset_frame(random_ix, random_ix)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]])).plot()\n",
    "plt.subplot(122)\n",
    "decode.plot.PlotFrameCoord(frame=frame_proc.forward(frames[[random_ix]]), \n",
    "                           pos_out=em_subset.xyz_px, phot_out=em_subset.prob).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a simple histogram rendering procedure for initial evaluation. You can specify the pixel size in nm, the sigma value for a Gaussian blur, an a clipping value for the brightness to control the color range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:39:36.867032Z",
     "start_time": "2020-10-10T16:39:35.134790Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "decode.renderer.Renderer2D(px_size=10., sigma_blur=5., clip_percentile=97.).render(emitter)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECODE will in general detect very high numbers of emitters, including those with high uncertainties.  \n",
    "Fortunately every DECODE prediction includes uncertainty estimates in x,y and z which we plot here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:39:37.684470Z",
     "start_time": "2020-10-10T16:39:36.868650Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,4))\n",
    "plt.subplot(131)\n",
    "plt.hist(emitter.xyz_sig_nm[:, 0].numpy())\n",
    "plt.xlabel('Sigma Estimate in X (nm)')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(emitter.xyz_sig_nm[:, 1].numpy())\n",
    "plt.xlabel('Sigma Estimate in Y (nm)')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.hist(emitter.xyz_sig_nm[:, 2].numpy())\n",
    "plt.xlabel('Sigma Estimate in Z (nm)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is almost always beneficial for the image quality to remove the worst localizations. \n",
    "Below we remove all localizations with uncertainties that exceed 40 nm in x,y or 80 nm in z.\n",
    "This leaves us with 74% of the detections. Alternatively you can specify the remaining percentage directly with the `emitter.filter_by_sigma(fraction)` method.\n",
    "If you compare the rendering to the previous (unfiltered) one, you can see that this procedure eliminated all visible artefacts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:39:39.641773Z",
     "start_time": "2020-10-10T16:39:37.685843Z"
    }
   },
   "outputs": [],
   "source": [
    "sigma_x_high_threshold = 40 \n",
    "sigma_y_high_threshold = 40 \n",
    "sigma_z_high_threshold = 80 \n",
    "\n",
    "em_sub = emitter[(emitter.xyz_sig_nm[:, 0] <= sigma_x_high_threshold) * (emitter.xyz_sig_nm[:, 1] <= sigma_x_high_threshold) * (emitter.xyz_sig_nm[:, 2] <= sigma_z_high_threshold)]\n",
    "# em_sub = emitter.filter_by_sigma(0.74)  # alternatively\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "decode.renderer.Renderer2D(px_size=10., sigma_blur=5., clip_percentile=97.).render(em_sub)\n",
    "plt.title(f'Filtered Image {np.round(100*len(em_sub)/len(emitter))} % of emitters remaining')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of specifying cutoffs for the three coordinates you can also use the filter_by_sigma method to directly specify what share of the emitters you wan't to keep.   \n",
    "Below we remove the 60% of emitters with the highest weighted combined uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:39:41.028239Z",
     "start_time": "2020-10-10T16:39:39.644060Z"
    }
   },
   "outputs": [],
   "source": [
    "em_sub = emitter.filter_by_sigma(0.4)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "decode.renderer.Renderer2D(px_size=10., sigma_blur=5., clip_percentile=97.).render(em_sub)\n",
    "plt.title(f'Filtered Image {np.round(100*len(em_sub)/len(emitter))} % of emitters remaining')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save EmitterSet\n",
    "There a different options to export your fitting results for later use. \n",
    "\n",
    "- The simplest one is saving it in CSV format which is compatible with most other frameworks. \n",
    "- Another option is saving via the native method `.save()` For this we use the backend of PyTorch, stored will be a dictionary of pytorch tensors as well as some meta information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:40:40.699199Z",
     "start_time": "2020-10-10T16:39:41.030048Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# csv export (loadable by most frameworks)\n",
    "decode.utils.emitter_io.save_csv('emitter.csv', emitter.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T16:40:44.003109Z",
     "start_time": "2020-10-10T16:40:40.702964Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fast native save\n",
    "emitter.save('emitter.pt')  # can be loaded via 'decode.EmitterSet.load('emitter.pt')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_env]",
   "language": "python",
   "name": "conda-env-decode_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
